# ğŸŒ ChatterEngine Serve Layer â€“ API Guide

This guide covers the `/serve/` directory, which provides a future interface for running ChatterEngine as a real-time service using **FastAPI** or **gRPC**.

---

## ğŸ“‚ Directory Overview

```
serve/
â”œâ”€â”€ api/              # FastAPI endpoints per module
â”‚   â”œâ”€â”€ stt_service.py
â”‚   â”œâ”€â”€ nmt_service.py
â”‚   â””â”€â”€ tts_service.py
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ bidirectional_handler.py
```

---

## ğŸš€ Goal

- Run ChatterEngine via REST or streaming API
- Support bidirectional flow: `sv â†’ en` and `en â†’ sv`
- Switch models based on query or route param

---

## ğŸ§  FastAPI Service Example: STT

```python
from fastapi import FastAPI, UploadFile
from stt.stt_module import load_stt_model, transcribe_audio

app = FastAPI()
stt_model, stt_cfg = load_stt_model("config/stt_config.yaml")

@app.post("/stt")
async def stt(file: UploadFile):
    audio_path = f"temp/{file.filename}"
    with open(audio_path, "wb") as f:
        f.write(await file.read())
    text = transcribe_audio(audio_path, stt_model, stt_cfg)
    return {"transcript": text}
```

---

## ğŸ“¦ Run the API

```bash
uvicorn serve.api.stt_service:app --reload --port 8000
```

You can run individual services or create a combined pipeline endpoint (see `bidirectional_handler.py`).

---

## ğŸ”„ Future Features

- Streaming audio input with WebSockets or gRPC
- Async queuing system (e.g. Celery)
- Model auto-selection from query (e.g. `lang=sv`)
- Live subtitle/translation stream

---

Build the API like the pipeline: modular, fast, language-flexible.
